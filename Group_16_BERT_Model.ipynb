{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Group_16_BERT_Model.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"e20c308a1b5440dcbc013154ad11a0ea":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0662b55a4fdf4e22a1dc0a600c2c6880","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3e7b847d4ddf4c22bdb8b75115d0286c","IPY_MODEL_93a63043230d4a3691c657090d75bed6"]}},"0662b55a4fdf4e22a1dc0a600c2c6880":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3e7b847d4ddf4c22bdb8b75115d0286c":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_fa52a282cd08445d99d6ad4d63083f56","_dom_classes":[],"description":"Downloading: 100%","_model_name":"IntProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_524556cd073948bb9575577e313c50c6"}},"93a63043230d4a3691c657090d75bed6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_53937d72c86c4ace90ca59e117df0bc3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:06&lt;00:00, 33.2kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e5ccc1ff83224f2b8e758924ce800f1b"}},"fa52a282cd08445d99d6ad4d63083f56":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"524556cd073948bb9575577e313c50c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"53937d72c86c4ace90ca59e117df0bc3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e5ccc1ff83224f2b8e758924ce800f1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"Egv_S32ONx1A","colab_type":"text"},"source":["The following notebook explores the BERT Model from the Hugging Face Library. We will fine tune the pretrained BERT Model on our data and then evaluate it on a test set. This is done using 5 fold cross validation and we have presented a confusion matrix along with various evaluation metrics to assess the performance of the model"]},{"cell_type":"markdown","metadata":{"id":"F-jgipUyBhrx","colab_type":"text"},"source":["# Data Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"U9qmBZQDBoAw","colab_type":"text"},"source":["## Import Libraries"]},{"cell_type":"code","metadata":{"id":"eLE1N96JbLhK","colab_type":"code","colab":{}},"source":["import sys\n","import time\n","import os\n","import math\n","import copy\n","import string\n","import re\n","from IPython.display import clear_output\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import torch\n","import torch.nn as nn\n","import torch.functional as F\n","\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset, TensorDataset\n","from torch.autograd import Variable"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ordCiMqAbS8r","colab_type":"code","outputId":"f886f927-b0ba-4ecf-e69e-c0f307cfd7d4","executionInfo":{"status":"ok","timestamp":1585911788057,"user_tz":-60,"elapsed":3930,"user":{"displayName":"Throw Away","photoUrl":"","userId":"04842125865463530966"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#We set our device to the GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","torch.cuda.get_device_name(0)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Tesla T4'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"l5NHZG90B9Xq","colab_type":"text"},"source":["Here we install the Hugging Face Library which contains the BERT Model"]},{"cell_type":"code","metadata":{"id":"iYFWzVY2bUHV","colab_type":"code","outputId":"9932fa3b-eb29-4ea5-823c-6607c6db8643","colab":{"base_uri":"https://localhost:8080/","height":712}},"source":["!pip install transformers"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/ba/dda44bbf35b071441635708a3dd568a5ca6bf29f77389f7c7c6818ae9498/transformers-2.7.0-py3-none-any.whl (544kB)\n","\r\u001b[K     |▋                               | 10kB 21.9MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20kB 27.3MB/s eta 0:00:01\r\u001b[K     |█▉                              | 30kB 30.7MB/s eta 0:00:01\r\u001b[K     |██▍                             | 40kB 33.1MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 35.0MB/s eta 0:00:01\r\u001b[K     |███▋                            | 61kB 37.6MB/s eta 0:00:01\r\u001b[K     |████▏                           | 71kB 38.6MB/s eta 0:00:01\r\u001b[K     |████▉                           | 81kB 39.9MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 92kB 40.9MB/s eta 0:00:01\r\u001b[K     |██████                          | 102kB 41.7MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 112kB 41.7MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 122kB 41.7MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 133kB 41.7MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 143kB 41.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 153kB 41.7MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 163kB 41.7MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 174kB 41.7MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 184kB 41.7MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 194kB 41.7MB/s eta 0:00:01\r\u001b[K     |████████████                    | 204kB 41.7MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 215kB 41.7MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 225kB 41.7MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 235kB 41.7MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 245kB 41.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 256kB 41.7MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 266kB 41.7MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 276kB 41.7MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 286kB 41.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 296kB 41.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 307kB 41.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 317kB 41.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 327kB 41.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 337kB 41.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 348kB 41.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 358kB 41.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 368kB 41.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 378kB 41.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 389kB 41.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 399kB 41.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 409kB 41.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 419kB 41.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 430kB 41.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 440kB 41.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 450kB 41.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 460kB 41.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 471kB 41.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 481kB 41.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 491kB 41.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 501kB 41.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 512kB 41.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 522kB 41.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 532kB 41.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 542kB 41.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 552kB 41.7MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n","Collecting tokenizers==0.5.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n","\u001b[K     |████████████████████████████████| 3.7MB 63.4MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.31)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n","\u001b[K     |████████████████████████████████| 870kB 71.5MB/s \n","\u001b[?25hCollecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 47.5MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n","Requirement already satisfied: botocore<1.16.0,>=1.15.31 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.31)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.31->boto3->transformers) (2.8.1)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.31->boto3->transformers) (0.15.2)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=8dd7ac3f88ad533a69f370a7483fb8458ae6d3eab3aac0503c2f61f6ceddfaf4\n","  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n","Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.7.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ghSHuvndcoAQ","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JRyl6N1XbUSj","colab_type":"code","colab":{}},"source":["\n","filepath = \"gdrive/My Drive/ted_training_pairs_NLTK2.csv\"\n","\n","data = pd.read_csv(filepath)\n","\n","sentences = data['text'].values\n","labels = data['label'].values\n","\n","#We wish to use the uncased model so we must ensure that all sentences are in lower case\n","\n","sentences = [sentences[i].lower() for i in range(len(sentences))]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sQ4Rp_QNbUW7","colab_type":"code","outputId":"696b1fc3-6ff4-4481-c040-2fc6aa5dd5d3","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#We encode the labels so that laughter is assigned \"0\" and neutral is assigned \"1\"\n","\n","from sklearn.preprocessing import LabelEncoder\n","\n","enc = LabelEncoder()\n","labels_enc = enc.fit_transform(labels)\n","\n","print(labels[0], labels_enc[0])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["laughter 0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tQ2EXkz3bUb9","colab_type":"code","outputId":"ce2f8289-a885-4c11-e244-178d7994c33e","colab":{"base_uri":"https://localhost:8080/","height":84,"referenced_widgets":["e20c308a1b5440dcbc013154ad11a0ea","0662b55a4fdf4e22a1dc0a600c2c6880","3e7b847d4ddf4c22bdb8b75115d0286c","93a63043230d4a3691c657090d75bed6","fa52a282cd08445d99d6ad4d63083f56","524556cd073948bb9575577e313c50c6","53937d72c86c4ace90ca59e117df0bc3","e5ccc1ff83224f2b8e758924ce800f1b"]}},"source":["from transformers import BertTokenizer\n","\n","#Load the BERT Tokenizer\n","print(\"Loading BERT tokenizer...\")\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Loading BERT tokenizer...\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e20c308a1b5440dcbc013154ad11a0ea","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HkHY6noIDsXk","colab_type":"text"},"source":["We will start by showing an example of how the BERT Model tokenizes a sentence"]},{"cell_type":"code","metadata":{"id":"Msh5-J0YbUgf","colab_type":"code","outputId":"12602093-6801-4cb0-e5fa-87bf2f2c6820","colab":{"base_uri":"https://localhost:8080/","height":90}},"source":["# Print the original sentence.\n","print(' Original: ', sentences[2])\n","\n","# Print the sentence split into tokens.\n","print('Tokenized: ', tokenizer.tokenize(sentences[2]))\n","\n","# Print the sentence mapped to token ids.\n","print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[2])))"],"execution_count":0,"outputs":[{"output_type":"stream","text":[" Original:  if you're at a dinner party , and you say you work in education — actually , you're not often at dinner parties , frankly . \n","Tokenized:  ['if', 'you', \"'\", 're', 'at', 'a', 'dinner', 'party', ',', 'and', 'you', 'say', 'you', 'work', 'in', 'education', '—', 'actually', ',', 'you', \"'\", 're', 'not', 'often', 'at', 'dinner', 'parties', ',', 'frankly', '.']\n","Token IDs:  [2065, 2017, 1005, 2128, 2012, 1037, 4596, 2283, 1010, 1998, 2017, 2360, 2017, 2147, 1999, 2495, 1517, 2941, 1010, 2017, 1005, 2128, 2025, 2411, 2012, 4596, 4243, 1010, 19597, 1012]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1nfUfEEcbUlQ","colab_type":"code","outputId":"45a04ecc-0977-4442-d14e-6db67c929d48","colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# We will now tokenize every sentence in our data\n","\n","sentence_tokens = []\n","\n","for sentence in sentences:\n","  encoded_sent = tokenizer.encode(sentence,\n","                                  add_special_tokens = True,\n","                                  )\n","  sentence_tokens.append(encoded_sent)\n","\n","print(\"Original: \", sentences[1])\n","print(\"Tokenized: \", sentence_tokens[1])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Original:  in fact , i'm leaving . \n","Tokenized:  [101, 1999, 2755, 1010, 1045, 1005, 1049, 2975, 1012, 102]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"INn4wKKJD_CS","colab_type":"text"},"source":["For the purpose of speeding up training as well as potentially improving training we will be truncating/padding all our sentences to a fixed length based on their sentence length distribution.\n","\n","Below we find the distribution of sentence lengths. We select a maximum length by choosing the length closest to the \"elbow\" of the distribution"]},{"cell_type":"code","metadata":{"id":"mo4PQ9EXbVB4","colab_type":"code","outputId":"35aac56c-5547-4e31-f4ce-ac3e104976c4","colab":{"base_uri":"https://localhost:8080/","height":424}},"source":["sentence_len = [len(x) for x in sentence_tokens]\n","pd.Series(sentence_len).hist()\n","plt.show()\n","pd.Series(sentence_len).describe()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkCRhGEiSMAwkSRgGkiTg/wGEZxvEEYBPswAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"text/plain":["count    17466.000000\n","mean        23.282950\n","std         16.873331\n","min          3.000000\n","25%         12.000000\n","50%         19.000000\n","75%         30.000000\n","max        224.000000\n","dtype: float64"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"1DgRPsnodt1W","colab_type":"code","outputId":"87e55930-4a52-4911-ffa0-e4eee1840af1","colab":{"base_uri":"https://localhost:8080/","height":141}},"source":["#By carefully analysing our data we choose a maximum legnth of 30 tokens\n","max_len = 30\n","\n","#We will use the keras padding function for convenience\n","from keras.preprocessing.sequence import pad_sequences\n","\n","print('\\nPadding/truncating all sentences to %d values...' % max_len)\n","\n","print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n","\n","# Pad our input tokens with value 0.\n","# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n","# as opposed to the beginning.\n","sentence_tokens = pad_sequences(sentence_tokens, maxlen=max_len, dtype=\"long\", \n","                          value=0, truncating=\"post\", padding=\"post\")\n","\n","print('\\nDone.')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Padding/truncating all sentences to 30 values...\n","\n","Padding token: \"[PAD]\", ID: 0\n","\n","Done.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"B4aUU3n_FClP","colab_type":"text"},"source":["As part of the input the BERT Model also requires an attention mask which will differentiate the sentence tokens from the padding tokens"]},{"cell_type":"code","metadata":{"id":"bG4BU4Fydt8H","colab_type":"code","colab":{}},"source":["# Create attention masks\n","attention_masks = []\n","\n","# For each sentence...\n","for sent in sentence_tokens:\n","    \n","    # Create the attention mask.\n","    #   - If a token ID is 0, then it's padding, set the mask to 0.\n","    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n","    att_mask = [int(token_id > 0) for token_id in sent]\n","    \n","    # Store the attention mask for this sentence.\n","    attention_masks.append(att_mask)\n","\n","attention_masks = np.array(attention_masks)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yw1_j7ZoduD6","colab_type":"code","outputId":"a2c20783-96de-4b00-8a43-be3c09e37a06","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["attention_masks.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(17466, 30)"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"fsMK6ZsKduKL","colab_type":"code","outputId":"6a980336-de35-4708-e805-086af966f36e","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["sentence_tokens.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(17466, 30)"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"hVcx-VUzezAv","colab_type":"code","colab":{}},"source":["# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    softmax_output = nn.functional.softmax(torch.tensor(preds), dim = 1)\n","    pred_flat = np.argmax(softmax_output, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(np.array(pred_flat) == labels_flat) / len(labels_flat)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"x1awpH3LezFi","colab_type":"code","colab":{}},"source":["import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vETPQZXWezJy","colab_type":"code","colab":{}},"source":["def train(model, iterator, optimizer, scheduler):\n","  epoch_loss = 0\n","  epoch_acc = 0\n","\n","  for batch in iterator:\n","    #Send outputs to the GPU if available\n","    b_input_ids = batch[0].to(device)\n","    b_input_mask = batch[1].to(device)\n","    b_labels = batch[2].to(device)\n","\n","    #Zero the gradient in our model\n","    model.zero_grad()\n","\n","    #Retrieve the outputs of the model\n","    outputs = model(b_input_ids, \n","                    token_type_ids=None, \n","                    attention_mask=b_input_mask, \n","                    labels=b_labels)\n","    \n","    #Compute loss\n","    loss = outputs[0]\n","    epoch_loss += loss.item()\n","\n","    #Calculate gradients \n","    loss.backward()\n","\n","    #Compute the accuracy\n","    logits = outputs[1]\n","\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","\n","    acc = flat_accuracy(logits, label_ids)\n","    epoch_acc += acc\n","\n","    #Gradient clip in order to avoid potential problem of\n","    #vanishing and exploding gradients\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","    #Take a step in the direction of the gradient\n","    optimizer.step()\n","\n","    #Take a step in the scheduler reducing learning rate\n","    scheduler.step()\n","\n","  return epoch_loss/len(iterator) , epoch_acc / len(iterator)\n","\n","def evaluate(model, iterator):\n","\n","  epoch_loss = 0\n","  epoch_acc = 0\n","\n","  with torch.no_grad():\n","\n","    for batch in iterator:\n","      #Send outputs to the GPU if available\n","      b_input_ids = batch[0].to(device)\n","      b_input_mask = batch[1].to(device)\n","      b_labels = batch[2].to(device)\n","\n","        \n","      outputs = model(b_input_ids, \n","                      token_type_ids=None, \n","                      attention_mask=b_input_mask,\n","                      labels=b_labels)\n","        \n","      #Compute loss\n","      loss = outputs[0]\n","      epoch_loss += loss.item()\n","      \n","      logits = outputs[1]\n","\n","      # Move logits and labels to CPU\n","      logits = logits.detach().cpu().numpy()\n","      label_ids = b_labels.to('cpu').numpy()\n","      \n","      # Calculate the accuracy for this batch of test sentences.\n","      acc = flat_accuracy(logits, label_ids)\n","      epoch_acc += acc\n","\n","  return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sUiV_XHQebD3","colab_type":"code","colab":{}},"source":["def plot_loss_accuracy(train_loss, valid_loss, train_acc, valid_acc):\n","    #plots the loss and accuracy of training and validation set during training\n","    # comment out to suppress plot output\n","    fig = plt.figure(figsize=plt.figaspect(0.2))\n","    ax1 = fig.add_subplot(1,2,1)\n","    ax1.plot(train_loss, 'b')\n","    ax1.plot(valid_loss, 'r')\n","    plt.xlabel('epoch')\n","    plt.ylabel('Loss')\n","    ax1.legend(['Train', 'Validation'])  \n","    \n","    ax1 = fig.add_subplot(1,2,2)\n","    ax1.plot(train_acc, 'b')\n","    ax1.plot(valid_acc, 'r')\n","    plt.xlabel('epoch')\n","    plt.ylabel('Accuracy')\n","    ax1.legend(['Train', 'Validation'])\n","\n","    ltrain = np.array(train_loss)  \n","    least_train_epoch = np.argmin(ltrain)\n","    least_train_loss = min(train_loss)\n","    print('Lowest training loss:',least_train_loss,'achieved at epoch:',least_train_epoch)\n","    \n","    lval = np.array(valid_loss)  \n","    least_val_epoch = np.argmin(lval)\n","    least_val_loss = min(valid_loss)\n","    print('Lowest validation loss:',least_val_loss,'achieved at epoch:',least_val_epoch)\n","    \n","    atrain = np.array(train_acc)  \n","    best_train_epoch = np.argmax(atrain)\n","    best_train_accuracy = max(train_acc)\n","    print('Best training accuracy:',best_train_accuracy * 100,'achieved at epoch:',best_train_epoch)\n","    aval = np.array(valid_acc)  \n","    best_val_epoch = np.argmax(aval)\n","    best_val_accuracy = max(valid_acc)\n","    print('Best validation accuracy:',best_val_accuracy*100,'achieved at epoch:',best_val_epoch,'\\n')\n","    \n","    return least_train_loss, least_val_loss, best_train_accuracy, best_val_accuracy"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j4m2EBXrxBY-","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import roc_auc_score,f1_score,confusion_matrix, accuracy_score\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\n","from transformers import get_linear_schedule_with_warmup\n","\n","# The Hugging Face Library authors reccomend that we use between 2 and 4 epochs for training\n","epochs = 3\n","\n","skf = StratifiedKFold(5)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4QyP55g7ajrN","colab_type":"text"},"source":["We will now run the training loop. We use 5-fold Cross Validation in order to compute averages and standard deviations of each metric considered"]},{"cell_type":"code","metadata":{"id":"ZNF4vtxweA2C","colab_type":"code","outputId":"ab784932-6b31-4713-80a5-bf19f708fafb","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["loss_list = []\n","f1_list = []\n","acc_list = []\n","auc_list = []\n","avloss_list = []\n","conf_matrices = []\n","\n","for trainval_indices, test_indices in skf.split(sentence_tokens, labels_enc):\n","  # Load BertForSequenceClassification, the pretrained BERT model with a single \n","  # linear classification layer on top. \n","  model = BertForSequenceClassification.from_pretrained(\n","      \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n","      num_labels = 2, # The number of output labels--2 for binary classification.\n","                      # You can increase this for multi-class tasks.   \n","      output_attentions = False, # Whether the model returns attentions weights.\n","      output_hidden_states = False, # Whether the model returns all hidden-states.\n","  )\n","  # Tell pytorch to run this model on the GPU.\n","  model.to(device)\n","\n","  # Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n","  # I believe the 'W' stands for 'Weight Decay fix\"\n","  optimizer = AdamW(model.parameters(),\n","                    lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n","                    eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                  )\n","\n","  from sklearn.model_selection import train_test_split\n","\n","  train_indices, val_indices = train_test_split(trainval_indices, train_size = 0.8, shuffle = True, random_state = 2020)\n","\n","  train_inputs = sentence_tokens[train_indices]\n","  train_masks = attention_masks[train_indices]\n","  train_labels = labels_enc[train_indices]\n","\n","  val_inputs = sentence_tokens[val_indices]\n","  val_masks = attention_masks[val_indices]\n","  val_labels = labels_enc[val_indices]\n","\n","  test_inputs = sentence_tokens[test_indices]\n","  test_masks = attention_masks[test_indices]\n","  test_labels = labels_enc[test_indices]\n","\n","  train_inputs = sentence_tokens[train_indices]\n","  train_masks = attention_masks[train_indices]\n","  train_labels = labels_enc[train_indices]\n","\n","  val_inputs = sentence_tokens[val_indices]\n","  val_masks = attention_masks[val_indices]\n","  val_labels = labels_enc[val_indices]\n","\n","  test_inputs = sentence_tokens[test_indices]\n","  test_masks = attention_masks[test_indices]\n","  test_labels = labels_enc[test_indices]\n","\n","  train_inputs[0]\n","\n","  # create Tensor datasets\n","  train_data = TensorDataset(torch.from_numpy(train_inputs), torch.from_numpy(train_masks), torch.from_numpy(train_labels))\n","  val_data = TensorDataset(torch.from_numpy(val_inputs), torch.from_numpy(val_masks), torch.from_numpy(val_labels))\n","  test_data = TensorDataset(torch.from_numpy(test_inputs), torch.from_numpy(test_masks), torch.from_numpy(test_labels))\n","  # dataloaders\n","  batch_size = 50\n","  # make sure to SHUFFLE your data\n","  train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n","  val_dataloader = DataLoader(val_data, shuffle=True, batch_size=batch_size)\n","  test_dataloader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n","\n","  # Total number of training steps is number of batches * number of epochs.\n","  total_steps = len(train_dataloader) * epochs\n","\n","  # Create the learning rate scheduler.\n","  scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                              num_warmup_steps = 0, # Default value in run_glue.py\n","                                              num_training_steps = total_steps)\n","  \n","  import random\n","\n","  # Set the seed value all over the place to make this reproducible.\n","  seed_val = 42\n","\n","  random.seed(seed_val)\n","  np.random.seed(seed_val)\n","  torch.manual_seed(seed_val)\n","  torch.cuda.manual_seed_all(seed_val)\n","\n","  # Store the average loss after each epoch so we can plot them.\n","  training_losses = []\n","  training_accs = []\n","\n","  val_losses = []\n","  val_accs = []\n","  total_val_loss = []\n","\n","  best_acc = 0\n","\n","  # For each epoch...\n","  for epoch_i in range(0, epochs):\n","      \n","      # ========================================\n","      #               Training\n","      # ========================================\n","      \n","      # Perform one full pass over the training set.\n","\n","      print(\"\")\n","      print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","      print('Training...')\n","\n","      # Measure how long the training epoch takes.\n","      t0 = time.time()\n","\n","      # Put the model into training mode\n","      model.train()\n","\n","      # For each batch of training data...\n","      train_loss, train_acc = train(model, train_dataloader, optimizer, scheduler)\n","      \n","      # Store the loss value for plotting the learning curve.\n","      training_losses.append(train_loss)\n","      training_accs.append(train_acc)\n","\n","      print(\"\")\n","      print(\"  Average training loss: {0:.2f}\".format(train_loss))\n","      print(\"  Average training accuracy: {0: .2f}\".format(train_acc))\n","      print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n","          \n","      # ========================================\n","      #               Validation\n","      # ========================================\n","      # After the completion of each training epoch, measure our performance on\n","      # our validation set.\n","\n","      print(\"\")\n","      print(\"Running Validation...\")\n","\n","      t0 = time.time()\n","\n","      # Put the model in evaluation mode--the dropout layers behave differently\n","      # during evaluation.\n","      model.eval()\n","\n","      val_loss, val_acc = evaluate(model, val_dataloader)\n","\n","      val_losses.append(val_loss)\n","      val_accs.append(val_acc)\n","\n","      # Report the final accuracy for this validation run.\n","      print(\"  Loss: {0:.2f}\".format(val_loss))\n","      print(\"  Accuracy: {0:.2f}\".format(val_acc))\n","      print(\"  Test took: {:}\".format(format_time(time.time() - t0)))\n","\n","      if val_acc > best_acc:\n","        best_acc = val_acc\n","\n","        output_dir = './model_save/'\n","\n","        # Create output directory if needed\n","        if not os.path.exists(output_dir):\n","            os.makedirs(output_dir)\n","\n","        model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n","        model_to_save.save_pretrained(output_dir)\n","        tokenizer.save_pretrained(output_dir)\n","\n","  print(\"\")\n","  print(\"Training complete!\\n\")\n","\n","  model = BertForSequenceClassification.from_pretrained(output_dir)\n","  model.to(device)\n","\n","  print('Predicting labels for {:,} test sentences...'.format(len(test_data)))\n","\n","  # Put model in evaluation mode\n","  model.eval()\n","\n","  # Tracking variables \n","  predictions, true_labels = [], []\n","  prediction_score = np.array([])\n","\n","  # Predict \n","  for batch in test_dataloader:\n","    # Add batch to GPU\n","    batch = tuple(t.to(device) for t in batch)\n","    \n","    # Unpack the inputs from our dataloader\n","    b_input_ids, b_input_mask, b_labels = batch\n","    \n","    # Telling the model not to compute or store gradients, saving memory and \n","    # speeding up prediction\n","    with torch.no_grad():\n","        # Forward pass, calculate logit predictions\n","        outputs = model(b_input_ids, token_type_ids=None, \n","                        attention_mask=b_input_mask)\n","\n","    logits = outputs[0]\n","\n","    # Move logits and labels to CPU\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","\n","    softmax_output = nn.functional.softmax(torch.tensor(logits), dim = 1)\n","    prediction_score = np.append(prediction_score, softmax_output[:, 1])\n","    \n","    # Store predictions and true labels\n","    predictions.append(logits)\n","    true_labels.append(label_ids)\n","\n","    val_loss, val_acc = evaluate(model, test_dataloader)\n","    total_val_loss.append(val_loss)\n","\n","\n","  # Combine the results across all batches. \n","  flat_predictions = np.concatenate(predictions, axis=0)\n","\n","  # For each sample, pick the label (0 or 1) with the higher score.\n","  flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","\n","  # Combine the correct labels for each batch into a single list.\n","  flat_true_labels = np.concatenate(true_labels, axis=0)\n","\n","  # Calculate the evaluation metrics\n","  f1 = f1_score(flat_true_labels, flat_predictions)\n","  acc = accuracy_score(flat_true_labels, flat_predictions)\n","  auc = roc_auc_score(flat_true_labels, prediction_score)\n","  avloss = np.array(total_val_loss).mean()\n","  eval_confusion_matrix = confusion_matrix(flat_true_labels, flat_predictions)\n","  conf_matrices.append(eval_confusion_matrix)\n","\n","\n","  print('F1: {:,}'.format(f1))\n","  print('Accuracy: {:,}'.format(acc))\n","  print('AUC: {:,}'.format(auc))\n","  print('Loss:{:,}'.format(avloss))\n","\n","  f1_list.append(f1)\n","  acc_list.append(acc)\n","  auc_list.append(auc)\n","  avloss_list.append(avloss)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","======== Epoch 1 / 3 ========\n","Training...\n","\n","  Average training loss: 0.63\n","  Average training accuracy:  0.64\n","  Training epcoh took: 0:01:15\n","\n","Running Validation...\n","  Loss: 0.60\n","  Accuracy: 0.67\n","  Test took: 0:00:05\n","\n","======== Epoch 2 / 3 ========\n","Training...\n","\n","  Average training loss: 0.53\n","  Average training accuracy:  0.75\n","  Training epcoh took: 0:01:16\n","\n","Running Validation...\n","  Loss: 0.61\n","  Accuracy: 0.69\n","  Test took: 0:00:05\n","\n","======== Epoch 3 / 3 ========\n","Training...\n","\n","  Average training loss: 0.44\n","  Average training accuracy:  0.81\n","  Training epcoh took: 0:01:16\n","\n","Running Validation...\n","  Loss: 0.65\n","  Accuracy: 0.68\n","  Test took: 0:00:05\n","\n","Training complete!\n","\n","Predicting labels for 3,494 test sentences...\n","F1: 0.6701268742791234\n","Accuracy: 0.6725815684029766\n","AUC: 0.6725815684029766\n","Loss:0.6242921901600701\n","\n","======== Epoch 1 / 3 ========\n","Training...\n","\n","  Average training loss: 0.64\n","  Average training accuracy:  0.63\n","  Training epcoh took: 0:01:17\n","\n","Running Validation...\n","  Loss: 0.61\n","  Accuracy: 0.66\n","  Test took: 0:00:05\n","\n","======== Epoch 2 / 3 ========\n","Training...\n","\n","  Average training loss: 0.55\n","  Average training accuracy:  0.72\n","  Training epcoh took: 0:01:17\n","\n","Running Validation...\n","  Loss: 0.60\n","  Accuracy: 0.68\n","  Test took: 0:00:05\n","\n","======== Epoch 3 / 3 ========\n","Training...\n","\n","  Average training loss: 0.47\n","  Average training accuracy:  0.79\n","  Training epcoh took: 0:01:17\n","\n","Running Validation...\n","  Loss: 0.64\n","  Accuracy: 0.68\n","  Test took: 0:00:05\n","\n","Training complete!\n","\n","Predicting labels for 3,493 test sentences...\n","F1: 0.6296529968454259\n","Accuracy: 0.6638992270254795\n","AUC: 0.6638728083030244\n","Loss:0.614953182850565\n","\n","======== Epoch 1 / 3 ========\n","Training...\n","\n","  Average training loss: 0.63\n","  Average training accuracy:  0.64\n","  Training epcoh took: 0:01:17\n","\n","Running Validation...\n","  Loss: 0.60\n","  Accuracy: 0.67\n","  Test took: 0:00:05\n","\n","======== Epoch 2 / 3 ========\n","Training...\n","\n","  Average training loss: 0.52\n","  Average training accuracy:  0.75\n","  Training epcoh took: 0:01:17\n","\n","Running Validation...\n","  Loss: 0.61\n","  Accuracy: 0.68\n","  Test took: 0:00:05\n","\n","======== Epoch 3 / 3 ========\n","Training...\n","\n","  Average training loss: 0.43\n","  Average training accuracy:  0.81\n","  Training epcoh took: 0:01:17\n","\n","Running Validation...\n","  Loss: 0.65\n","  Accuracy: 0.68\n","  Test took: 0:00:05\n","\n","Training complete!\n","\n","Predicting labels for 3,493 test sentences...\n","F1: 0.691009681881051\n","Accuracy: 0.6802175780131692\n","AUC: 0.6802276329049768\n","Loss:0.6731214076280594\n","\n","======== Epoch 1 / 3 ========\n","Training...\n","\n","  Average training loss: 0.64\n","  Average training accuracy:  0.64\n","  Training epcoh took: 0:01:17\n","\n","Running Validation...\n","  Loss: 0.59\n","  Accuracy: 0.67\n","  Test took: 0:00:05\n","\n","======== Epoch 2 / 3 ========\n","Training...\n","\n","  Average training loss: 0.53\n","  Average training accuracy:  0.74\n","  Training epcoh took: 0:01:17\n","\n","Running Validation...\n","  Loss: 0.59\n","  Accuracy: 0.69\n","  Test took: 0:00:05\n","\n","======== Epoch 3 / 3 ========\n","Training...\n","\n","  Average training loss: 0.44\n","  Average training accuracy:  0.80\n","  Training epcoh took: 0:01:17\n","\n","Running Validation...\n","  Loss: 0.63\n","  Accuracy: 0.69\n","  Test took: 0:00:05\n","\n","Training complete!\n","\n","Predicting labels for 3,493 test sentences...\n","F1: 0.697524219590958\n","Accuracy: 0.6782135699971371\n","AUC: 0.6781953484651483\n","Loss:0.6386876263788767\n","\n","======== Epoch 1 / 3 ========\n","Training...\n","\n","  Average training loss: 0.64\n","  Average training accuracy:  0.63\n","  Training epcoh took: 0:01:17\n","\n","Running Validation...\n","  Loss: 0.60\n","  Accuracy: 0.68\n","  Test took: 0:00:05\n","\n","======== Epoch 2 / 3 ========\n","Training...\n","\n","  Average training loss: 0.54\n","  Average training accuracy:  0.73\n","  Training epcoh took: 0:01:17\n","\n","Running Validation...\n","  Loss: 0.60\n","  Accuracy: 0.69\n","  Test took: 0:00:05\n","\n","======== Epoch 3 / 3 ========\n","Training...\n","\n","  Average training loss: 0.44\n","  Average training accuracy:  0.80\n","  Training epcoh took: 0:01:17\n","\n","Running Validation...\n","  Loss: 0.63\n","  Accuracy: 0.69\n","  Test took: 0:00:05\n","\n","Training complete!\n","\n","Predicting labels for 3,493 test sentences...\n","F1: 0.7128603104212861\n","Accuracy: 0.7034068136272545\n","AUC: 0.7033974458587492\n","Loss:0.5956066484962194\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"o4IUPZanVS7_","colab_type":"code","outputId":"e1226fa6-9155-4af9-acbc-137f3a0020a2","colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["print(f'\\tMean Test Acc: {np.array(acc_list).mean()*100:.2f}% | Std Train Acc: {np.array(acc_list).std():.3f}')\n","print(f'\\tMean Test F1: {np.array(f1_list).mean():.3f} | Std Test F1: {np.array(f1_list).std():.3f}')\n","print(f'\\tMean Test AUC: {np.array(auc_list).mean():.3f} | Std Test AUC: {np.array(auc_list).std():.3f}')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\tMean Test Acc: 67.97% | Std Train Acc: 0.013\n","\tMean Test F1: 0.680 | Std Test F1: 0.029\n","\tMean Test AUC: 0.680 | Std Test AUC: 0.013\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RvLjPP3UcPCn","colab_type":"text"},"source":["For every run through the test set we compute a confusion matrix. From all these matrices we can compute the mean confusions with their standard deviation"]},{"cell_type":"code","metadata":{"id":"ruGRnWgZRevs","colab_type":"code","outputId":"ddc635e6-ef92-43f5-ae60-bdffd9803f7c","colab":{"base_uri":"https://localhost:8080/","height":527}},"source":["mean_confusions = np.mean(np.array(conf_matrices), axis = 0)\n","std_confusions = np.std(np.array(conf_matrices), axis = 0)\n","\n","label_names = [\"Laughter\", \"Neutral\"]\n","\n","fig, ax = plt.subplots(figsize = (8, 8))\n","im = ax.imshow(mean_confusions, cmap = \"Blues\")\n","\n","# We want to show all ticks...\n","ax.set_xticks(np.arange(len(label_names)))\n","ax.set_yticks(np.arange(len(label_names)))\n","# ... and label them with the respective list entries\n","ax.set_xticklabels(label_names)\n","ax.set_yticklabels(label_names)\n","\n","# Rotate the tick labels and set their alignment.\n","plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n","         rotation_mode=\"anchor\")\n","plt.xlabel(\"Predicted Labels\", fontsize = 'large')\n","plt.ylabel(\"True Labels\", fontsize = 'large')\n","# Loop over data dimensions and create text annotations.\n","for i in range(len(label_names)):\n","    for j in range(len(label_names)):\n","        text = ax.text(j, i, f'{mean_confusions[i, j]:.3f} ± {std_confusions[i, j]:.3f}',\n","                       ha=\"center\", va=\"center\", color=\"black\")\n","\n","ax.set_title(\"Confusion Matrix\")\n","fig.tight_layout()\n","plt.colorbar(im)\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAiwAAAH/CAYAAAB0NYb1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 576x576 with 2 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"GRRVQo06IV-B","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}