{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"Group_16_Logistic_Regression_additional.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"E96NtnCXi1Wy","colab_type":"text"},"source":["This Notebook trains and evaluates (with CV) a Bag-of-words TF-IDF logistic regression model. Additional features, polarity, subjectivity and incongruity are added.\n","\n","Group:16"]},{"cell_type":"code","metadata":{"id":"NodAQD98PQKt","colab_type":"code","colab":{}},"source":["import sys\n","import time\n","import os\n","import math\n","import copy\n","import string\n","import re\n","from IPython.display import clear_output\n","import nltk\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from textblob import TextBlob\n","import torch\n","import torch.nn as nn\n","import torch.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset, TensorDataset\n","from torch.autograd import Variable\n","\n","import torchvision\n","import torchvision.transforms as transforms\n","from torchvision.utils import save_image\n","\n","import gensim\n","from gensim.models import Word2Vec\n","from gensim.models import KeyedVectors\n","from sklearn.model_selection import train_test_split"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"py9SjcroPQKx","colab_type":"code","colab":{}},"source":["# utilising the gpu\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JblO-LA3PZEY","colab_type":"code","outputId":"755279f3-90bb-4021-a83e-760aa92bfda6","colab":{"base_uri":"https://localhost:8080/","height":17}},"source":["%%shell\n","curl -fsS https://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip -o /tmp/trainDevTestTrees_PTB.zip\n","unzip -q -o -d /tmp /tmp/trainDevTestTrees_PTB.zip\n","rm -f /tmp/trainDevTestTrees_PTB.zip"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":[""]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"s1JsRYQLPdLc","colab_type":"code","outputId":"3984fb37-0194-43ee-d234-7b9f44eba2db","colab":{"base_uri":"https://localhost:8080/","height":221}},"source":["# https://stackoverflow.com/questions/55444572/how-does-i-unzip-pretrained-word2vec-in-google-colab\n","start_time = time.time()\n","!wget -P /tmp/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n","elapsed_time = time.time() - start_time\n","print('Downloading the pre-trained Word2Vec model took %d seconds' %(elapsed_time))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2020-03-26 15:09:35--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n","Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.76.222\n","Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.76.222|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1647046227 (1.5G) [application/x-gzip]\n","Saving to: ‘/tmp/GoogleNews-vectors-negative300.bin.gz’\n","\n","GoogleNews-vectors- 100%[===================>]   1.53G  67.0MB/s    in 23s     \n","\n","2020-03-26 15:09:59 (68.3 MB/s) - ‘/tmp/GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n","\n","Downloading the pre-trained Word2Vec model took 25 seconds\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4gm40NWqPQK0","colab_type":"code","outputId":"ca3ab3d3-602e-41aa-9935-0b6cb67f0503","colab":{"base_uri":"https://localhost:8080/","height":88}},"source":["# Load Google's pre-trained Word2Vec model.\n","start_time = time.time()\n","word2Vec_filePath = '/tmp/GoogleNews-vectors-negative300.bin.gz'\n","model = gensim.models.KeyedVectors.load_word2vec_format(word2Vec_filePath, binary=True)\n","!rm -f word2Vec_filePath\n","elapsed_time = time.time() - start_time\n","print('Loading the pre-trained Word2Vec model took %d seconds' %(elapsed_time))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:410: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"},{"output_type":"stream","text":["Loading the pre-trained Word2Vec model took 108 seconds\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iYQEFLsj3Xd6","colab_type":"text"},"source":["### Cleaning the Sentences"]},{"cell_type":"markdown","metadata":{"id":"BMNN8pg3PQK4","colab_type":"text"},"source":["Split data into sentences and labels"]},{"cell_type":"code","metadata":{"id":"yKjOOA6xkNAc","colab_type":"code","colab":{}},"source":["from google.colab import drive, files\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z39j3lOxPQK4","colab_type":"code","colab":{}},"source":["filepath = \"gdrive/My Drive/ted_training_pairs_NLTK2.csv\"\n","\n","data = pd.read_csv(filepath)\n","\n","sentences = data['text'].values\n","labels = data['label'].values"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_cPucRfGPQK_","colab_type":"code","outputId":"4b97f6cc-4488-4395-db63-a9abb25d70d1","colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["labels[0]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'laughter'"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"YM1MwPErPQLD","colab_type":"text"},"source":["Remove punctuation from each sentence"]},{"cell_type":"code","metadata":{"id":"DCPiAZUTPQLD","colab_type":"code","colab":{}},"source":["#Create list of all punctuation in the text\n","punctuation = []\n","for p in string.punctuation:\n","    punctuation.append(p)\n","punctuation.append(\"''\")\n","punctuation.append(\"--\")\n","punctuation.append(\"##\")\n","punctuation.append(\"``\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jkMAapVdPQLH","colab_type":"code","colab":{}},"source":["new_sentences = []\n","for sentence in sentences:\n","    words = sentence.split()\n","    no_punc_words = [words[i] for i in range(len(words)) if words[i] not in punctuation]\n","    new_sentences.append(\" \".join(no_punc_words))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6WvBBC3-PQLJ","colab_type":"text"},"source":["Creat a dictionary of the vocabulary in order of most frequent words"]},{"cell_type":"code","metadata":{"id":"UUz05s8NPQLK","colab_type":"code","colab":{}},"source":["from collections import Counter\n","\n","all_text = ' '.join(new_sentences)\n","words = all_text.split()\n","\n","# Count all the words using Counter Method\n","count_words = Counter(words)\n","\n","total_words = len(words)\n","sorted_words = count_words.most_common(total_words)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QBEuTVxgzls6","colab_type":"text"},"source":["This is where we start with BoW/ TfIdf model"]},{"cell_type":"code","metadata":{"id":"cP292ZCVgT3a","colab_type":"code","colab":{}},"source":["# Only keep words that appear more than once\n","processed_corpus = [[token for token in text.split() if count_words[token] > 1] for text in new_sentences]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AH-gC0h-egAI","colab_type":"code","colab":{}},"source":["from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n","# create model\n","vect_word = TfidfVectorizer(max_features=2000, lowercase=True, analyzer='word',\n","                        stop_words= 'english',ngram_range=(1,3),dtype=np.float32)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kAXNUwwVeoCz","colab_type":"code","colab":{}},"source":["tr_vect = vect_word.fit_transform(sentences)#apply model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X1sg_QPRfEB0","colab_type":"code","colab":{}},"source":["# transform to pytorch Tensor via sparse matrix\n","coo = tr_vect.tocoo()\n","values = coo.data\n","indices = np.vstack((coo.row, coo.col))\n","\n","i = torch.LongTensor(indices)\n","v = torch.FloatTensor(values)\n","shape = coo.shape\n","\n","encoded  = torch.sparse.FloatTensor(i, v, torch.Size(shape)).to_dense()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iXD9ayV7jc06","colab_type":"code","outputId":"08805ebd-938b-46dc-8bd0-278b3562cf8d","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["encoded.size()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([17466, 2000])"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"hpMMCieaaiKa","colab_type":"text"},"source":["Adding polarity and subjectivity"]},{"cell_type":"code","metadata":{"id":"i3WBOYoFaiw_","colab_type":"code","colab":{}},"source":["pol = lambda x: TextBlob(x).sentiment.polarity\n","sub = lambda x: TextBlob(x).sentiment.subjectivity"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B_PPOI81ai6Z","colab_type":"code","colab":{}},"source":["spol = (torch.FloatTensor(data['text'].apply(pol).values).unsqueeze(1) + 1.)/2.\n","ssub = torch.FloatTensor(data['text'].apply(sub).values).unsqueeze(1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"356bhFTlaoUW","colab_type":"text"},"source":["Adding Incongruity"]},{"cell_type":"code","metadata":{"id":"fo6AvCMpaolI","colab_type":"code","colab":{}},"source":["\n","def get_incongruity_scores(sentence):\n","  min_similarity = 1\n","  for word1 in sentence:\n","      for word2 in sentence:\n","        if word1 != word2:\n","          try:\n","            similarity = model.similarity(word1, word2)\n","            if similarity < min_similarity:\n","              min_similarity = similarity\n","          except Exception:\n","            pass\n","  max_similarity = -1\n","  for word1 in sentence:\n","      for word2 in sentence:\n","        if word1 != word2:\n","          try:\n","            similarity = model.similarity(word1, word2)\n","            if similarity > max_similarity:\n","              max_similarity = similarity\n","          except Exception:\n","            pass\n","\n","  if min_similarity == 1:\n","    min_similarity == 0\n","  if max_similarity == -1:\n","    max_similarity == 0\n","  \n","  return min_similarity, max_similarity"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GqgAbIQOa8Ik","colab_type":"code","outputId":"c4ef9664-0152-4c4f-f5d7-56f63bfac406","colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["sincro = [get_incongruity_scores(x) for x in processed_corpus]# get incongruity scores for each sentence"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n","  if np.issubdtype(vec.dtype, np.int):\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"vKjiRff8rrdG","colab_type":"code","colab":{}},"source":["sincro1 = (np.array([*sincro]) +1)/2 # normalise scores to [0,1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jNWhxF-ofQ7n","colab_type":"code","colab":{}},"source":["encoded2 = torch.cat([encoded,spol,ssub,torch.FloatTensor(sincro1)],1) # concatenate together"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N8OtL3uzPQLb","colab_type":"text"},"source":["Now we will tokenize the labels"]},{"cell_type":"code","metadata":{"id":"getvYrTYPQLc","colab_type":"code","colab":{}},"source":["from sklearn.preprocessing import LabelEncoder\n","\n","enc = LabelEncoder()\n","labels_enc = enc.fit_transform(labels)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yZ-_E3QXPQLf","colab_type":"code","outputId":"796e883e-b13e-4244-d320-dee4f181e49b","colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["labels_enc[0]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"V9q-Q4Gmkuc2","colab":{}},"source":["indices = list(range(encoded.shape[0]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i-HnZ1SsPQL6","colab_type":"text"},"source":["We will now design the Logistic Regression model"]},{"cell_type":"code","metadata":{"id":"SfVlQkn2z_MT","colab_type":"code","colab":{}},"source":["### Linear Model for TfIDf where embeddings are precomputed and are fed in as vectors so no need for embeddings layer in model\n","### note that a number of the inputs are not necessary. This is just for compatibility with other models.\n","class LR(nn.Module):\n","    def __init__(self, embedding_dim, n_filters, filter_sizes, output_dim, \n","                 dropout):\n","        \n","        super().__init__()\n","        \n","        self.fc = nn.Linear(embedding_dim, output_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, text):\n","                \n","            \n","        return self.fc(text)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ix2aJBfQPQL-","colab_type":"text"},"source":["Function to train the model"]},{"cell_type":"code","metadata":{"id":"zH5Dcra8PQL-","colab_type":"code","colab":{}},"source":["from sklearn.metrics import roc_auc_score,f1_score,confusion_matrix\n","\n","def binary_accuracy(preds, y):\n","\n","    #round predictions to the closest integer\n","    probs = torch.sigmoid(preds)\n","    rounded_preds = torch.round(probs)\n","    correct = (rounded_preds == y).float() #convert into float for division \n","    acc = correct.sum() / len(correct)\n","\n","    return acc"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j8R8W380PQMB","colab_type":"code","colab":{}},"source":["\n","\n","def train(model, iterator, optimizer, criterion):\n","    # model training function - performs one epoch\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    epoch_f1 = 0\n","    epoch_auc = 0\n","    pred_probs = np.array([])\n","    preds      = np.array([])\n","    labs = np.array([])\n","    model.train()\n","    \n","    for text, labels in iterator:\n","        \n","        text, labels = text.to(device), labels.to(device)\n","        \n","        optimizer.zero_grad()\n","        # forward\n","        predictions = model(text).squeeze(1)\n","        \n","        loss = criterion(predictions, labels)\n","        \n","        acc = binary_accuracy(predictions, labels)\n","        # collect probabilities and predictions\n","        pred_prob = torch.sigmoid(predictions)\n","        pred_probs = np.append(pred_probs, pred_prob.detach().cpu())\n","        pred = torch.round(pred_prob)\n","        preds = np.append(preds, pred.detach().cpu())\n","        labs = np.append(labs,labels.detach().cpu())\n","        # backward\n","        loss.backward()\n","        # optimise\n","        optimizer.step()\n","        \n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","\n","    #f1 \n","    f1 =  f1_score(labs,preds)\n","    #auc\n","    auc = roc_auc_score(labs,pred_probs) \n","    # return metrics for epoch    \n","    return epoch_loss/len(iterator) , epoch_acc / len(iterator), f1, auc\n","\n","def evaluate(model, iterator, criterion):\n","    # evaluate of test data. same as train but without model updating\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    epoch_f1 = 0\n","    epoch_auc = 0\n","    pred_probs = np.array([])\n","    preds      = np.array([])\n","    labs = np.array([])\n","    model.eval()\n","    \n","    with torch.no_grad():\n","    \n","        for text, labels in iterator:\n","            \n","            text, labels = text.to(device), labels.to(device)\n","\n","            predictions = model(text).squeeze(1)\n","            \n","            loss = criterion(predictions, labels)\n","            \n","            acc = binary_accuracy(predictions, labels)\n","\n","            pred_prob = torch.sigmoid(predictions)\n","            pred_probs = np.append(pred_probs, pred_prob.detach().cpu())\n","            pred = torch.round(pred_prob)\n","            preds = np.append(preds, pred.detach().cpu())\n","            labs = np.append(labs,labels.detach().cpu())\n","\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","\n","    #f1\n","    f1 =  f1_score(labs,preds)\n","    #auc \n","    auc = roc_auc_score(labs,pred_probs)    \n","    return epoch_loss/len(iterator) , epoch_acc / len(iterator), f1, auc, preds\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"91p81nVSPQMD","colab_type":"code","colab":{}},"source":["def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jMoS0voJPQMG","colab_type":"text"},"source":["Helper function to make plots of the loss and accuracy on the training set and validation set"]},{"cell_type":"code","metadata":{"id":"YYjzmzJmPQMH","colab_type":"code","colab":{}},"source":["def plot_loss_accuracy(train_loss, valid_loss, train_acc, valid_acc):\n","    #plots the loss and accuracy of training and validation set during training\n","    # comment out to suppress plot output\n","    fig = plt.figure(figsize=plt.figaspect(0.2))\n","    ax1 = fig.add_subplot(1,2,1)\n","    ax1.plot(train_loss, 'b')\n","    ax1.plot(valid_loss, 'r')\n","    plt.xlabel('epoch')\n","    plt.ylabel('Loss')\n","    ax1.legend(['Train', 'Validation'])  \n","    \n","    ax1 = fig.add_subplot(1,2,2)\n","    ax1.plot(train_acc, 'b')\n","    ax1.plot(valid_acc, 'r')\n","    plt.xlabel('epoch')\n","    plt.ylabel('Accuracy')\n","    ax1.legend(['Train', 'Validation'])\n","\n","    ltrain = np.array(train_loss)  \n","    least_train_epoch = np.argmin(ltrain)\n","    least_train_loss = min(train_loss)\n","    print('Lowest training loss:',least_train_loss,'achieved at epoch:',least_train_epoch)\n","    \n","    lval = np.array(valid_loss)  \n","    least_val_epoch = np.argmin(lval)\n","    least_val_loss = min(valid_loss)\n","    print('Lowest validation loss:',least_val_loss,'achieved at epoch:',least_val_epoch)\n","    \n","    atrain = np.array(train_acc)  \n","    best_train_epoch = np.argmax(atrain)\n","    best_train_accuracy = max(train_acc)\n","    print('Best training accuracy:',best_train_accuracy * 100,'achieved at epoch:',best_train_epoch)\n","    aval = np.array(valid_acc)  \n","    best_val_epoch = np.argmax(aval)\n","    best_val_accuracy = max(valid_acc)\n","    print('Best validation accuracy:',best_val_accuracy*100,'achieved at epoch:',best_val_epoch,'\\n')\n","    \n","    return least_train_loss, least_val_loss, best_train_accuracy, best_val_accuracy"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"50OaIH8FA0Wm","colab_type":"text"},"source":["Grid Search"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"hL7WH8RlPQMP","colab_type":"code","outputId":"4002b47b-b77c-4916-b0fe-c080267059bf","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["EMBEDDING_DIM = encoded2.shape[1]\n","N_FILTERS = 100 #<- not applicable but necessary \n","N_FILTERS_list = [50, 100, 150] #<- not applicable but necessary \n","FILTER_SIZES = [3,4,5] #<- not applicable but necessary \n","FILTER_SIZES_list = [[3, 4, 5], [2, 3, 4]] #<- not applicable but necessary \n","OUTPUT_DIM = 1\n","DROPOUT_list = [0.3, 0.5, 0.7] #<- not applicable but necessary \n","DROPOUT = 0.5 #<- not applicable but necessary \n","learning_rates = [0.0001, 0.001, 0.01]\n","batch_sizes = [25,50]\n","best_valid_loss = float('inf')\n","from sklearn.model_selection import StratifiedKFold\n","\n","# Here is where we implement the CV\n","\n","skf1 = StratifiedKFold(5)\n","\n","# averages over the test set\n","test_loss_avg = []\n","test_acc_avg = []\n","test_auc_avg = []\n","test_f1_avg = []\n","\n","\n","train_loss_avg = []\n","train_acc_avg = []\n","train_auc_avg = []\n","train_f1_avg = []\n","\n","conf_matrices = []\n","#  CV over 5 train test splits\n","for train_indices_m, test_indices in skf1.split(indices,labels_enc):\n","\n","  \n","  \n","\n","  skf2 = StratifiedKFold(5)\n","  # averages over train and validation splits\n","  train_losses_avg = np.zeros((len(batch_sizes),len(learning_rates)))\n","  train_accs_avg = np.zeros((len(batch_sizes),len(learning_rates)))\n","  train_aucs_avg = np.zeros((len(batch_sizes),len(learning_rates)))\n","  train_f1s_avg = np.zeros((len(batch_sizes),len(learning_rates)))\n","\n","  train_losses_std = np.zeros((len(batch_sizes),len(learning_rates)))\n","  train_accs_std = np.zeros((len(batch_sizes),len(learning_rates)))\n","  train_aucs_std = np.zeros((len(batch_sizes),len(learning_rates)))\n","  train_f1s_std = np.zeros((len(batch_sizes),len(learning_rates)))\n","\n","\n","  val_losses_avg = np.zeros((len(batch_sizes),len(learning_rates)))\n","  val_accs_avg = np.zeros((len(batch_sizes),len(learning_rates)))\n","  val_aucs_avg = np.zeros((len(batch_sizes),len(learning_rates)))\n","  val_f1s_avg = np.zeros((len(batch_sizes),len(learning_rates)))\n","\n","  val_losses_std = np.zeros((len(batch_sizes),len(learning_rates)))\n","  val_accs_std = np.zeros((len(batch_sizes),len(learning_rates)))\n","  val_aucs_std = np.zeros((len(batch_sizes),len(learning_rates)))\n","  val_f1s_std = np.zeros((len(batch_sizes),len(learning_rates)))\n","\n","\n","  # grid search over params\n","  for i,batch_size in enumerate(batch_sizes):\n","\n","    for j,lr in enumerate(learning_rates):\n","      \n","      # losses will be averaged\n","      train_losses = []\n","      train_accs = []\n","      train_f1s = []\n","      train_aucs = []\n","\n","      val_losses = []\n","      val_accs = []\n","      val_f1s = []\n","      val_aucs = []\n","\n","      # CV for each parameter combination\n","      for train_indices,val_indices in skf2.split(train_indices_m,labels_enc[train_indices_m]):\n","\n","\n","        # now split train and test\n","        X_train = encoded2[train_indices]\n","        y_train = labels_enc[train_indices]\n","\n","        X_val = encoded2[val_indices]\n","        y_val = labels_enc[val_indices]\n","\n","        # create Tensor datasets as usual\n","        train_data = TensorDataset(X_train, torch.from_numpy(y_train).type(torch.FloatTensor))\n","        valid_data = TensorDataset(X_val, torch.from_numpy(y_val).type(torch.FloatTensor))\n","        \n","        # dataloaders\n","        \n","        # make sure to SHUFFLE your data\n","        train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last = False)\n","        valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size, drop_last = False)\n","        \n","        # instantiate model\n","        model = LR(EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT).to(device)\n","\n","        # loss function - using cross entropy loss\n","        criterion = nn.BCEWithLogitsLoss().to(device)\n","        optimizer = torch.optim.Adam(model.parameters(), lr= lr)\n","\n","        N_EPOCHS = 12\n","\n","        e_train_loss = np.inf\n","        e_train_acc = 0\n","        e_train_f1 = 0\n","        e_train_auc = 0\n","        \n","        e_valid_loss = np.inf\n","        e_valid_acc = 0\n","        e_valid_f1 = 0\n","        e_valid_auc = 0\n","\n","        for epoch in range(N_EPOCHS): # train for N epochs\n","\n","          start_time = time.time()\n","          \n","          train_loss, train_acc, train_f1, train_auc = train(model, train_loader, optimizer, criterion)\n","          valid_loss, valid_acc, valid_f1, valid_auc,_ = evaluate(model, valid_loader, criterion)\n","          \n","          if valid_acc > e_valid_acc:\n","            e_train_loss = train_loss\n","            e_train_acc = train_acc\n","            e_train_f1 = train_f1\n","            e_train_auc = train_auc\n","            \n","            e_valid_loss = valid_loss\n","            e_valid_acc = valid_acc\n","            e_valid_f1 = valid_f1\n","            e_valid_auc = valid_auc\n","          \n","          \n","          end_time = time.time()\n","\n","          epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","        \n","          print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","          print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","          print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n","        clear_output(True)\n","        # save best epoch metrics for averaging\n","        train_losses.append(e_train_loss)\n","        train_accs.append(e_train_acc)\n","        train_f1s.append(e_train_f1)\n","        train_aucs.append(e_train_auc)\n","        \n","        val_losses.append(e_valid_loss)\n","        val_accs.append(e_valid_acc)\n","        val_f1s.append(e_valid_f1)\n","        val_aucs.append(e_valid_auc)\n","      # average best metrics for each parameter combination\n","      train_losses_avg[i,j] = np.array(train_losses).mean()\n","      train_accs_avg[i,j] = np.array(train_accs).mean()\n","      train_aucs_avg[i,j] = np.array(train_aucs).mean()\n","      train_f1s_avg[i,j] = np.array(train_f1s).mean()\n","\n","      train_losses_std[i,j] = np.array(train_losses).std()\n","      train_accs_std[i,j] = np.array(train_accs).std()\n","      train_aucs_std[i,j] = np.array(train_aucs).std()\n","      train_f1s_std[i,j] = np.array(train_f1s).std()\n","\n","\n","      val_losses_avg[i,j] = np.array(train_losses).mean()\n","      val_accs_avg[i,j] = np.array(train_accs).mean()\n","      val_aucs_avg[i,j] = np.array(train_aucs).mean()\n","      val_f1s_avg[i,j] = np.array(train_f1s).mean()\n","\n","      val_losses_std[i,j] = np.array(train_losses).std()\n","      val_accs_std[i,j] = np.array(train_accs).std()\n","      val_aucs_std[i,j] = np.array(train_aucs).std()\n","      val_f1s_std[i,j] = np.array(train_f1s).std()\n","\n","  # find best parameter combination\n","  i,j = np.unravel_index(np.argmin(val_losses_avg),val_losses_avg.shape)\n","  best_batch = batch_sizes[i]\n","  best_lr = learning_rates[j]\n","  # instantiate new model\n","  model = LR(EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT).to(device)\n","\n","  # loss function - using cross entropy loss\n","  criterion = nn.BCEWithLogitsLoss().to(device)\n","  optimizer = torch.optim.Adam(model.parameters(), lr= best_lr)\n","  # new train val split\n","  train_indices, valid_indices = train_test_split(train_indices_m, test_size = 0.2, shuffle = True)  \n","\n","  X_train = encoded2[train_indices]\n","  y_train = labels_enc[train_indices]\n","\n","  X_val = encoded2[valid_indices]\n","  y_val = labels_enc[valid_indices]\n","  # create test set here too\n","  X_test = encoded2[test_indices]\n","  y_test = labels_enc[test_indices]\n","\n","  train_data = TensorDataset(X_train, torch.from_numpy(y_train).type(torch.FloatTensor))\n","  valid_data = TensorDataset(X_val, torch.from_numpy(y_val).type(torch.FloatTensor))\n","  test_data = TensorDataset(X_test, torch.from_numpy(y_test).type(torch.FloatTensor))\n","\n","  train_loader = DataLoader(train_data, shuffle=True, batch_size=best_batch, drop_last = False)\n","  valid_loader = DataLoader(valid_data, shuffle=True, batch_size=best_batch, drop_last = False)\n","  test_loader = DataLoader(test_data, shuffle=False, batch_size=best_batch, drop_last = False)\n","\n","  N_EPOCHS = 12\n","\n","  e_train_loss = np.inf\n","  e_train_acc = 0\n","  e_train_f1 = 0\n","  e_train_auc = 0\n","  \n","  e_valid_loss = np.inf\n","  e_valid_acc = 0\n","  e_valid_f1 = 0\n","  e_valid_auc = 0\n","  # train\n","  for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","    \n","    train_loss, train_acc, train_f1, train_auc = train(model, train_loader, optimizer, criterion)\n","    valid_loss, valid_acc, valid_f1, valid_auc,_ = evaluate(model, valid_loader, criterion)\n","\n","    \n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    if valid_loss < e_valid_loss:\n"," \n","      e_train_loss = train_loss\n","      e_train_acc = train_acc\n","      e_train_f1 = train_f1\n","      e_train_auc = train_auc\n","      \n","      e_valid_loss = valid_loss\n","      e_valid_acc = valid_acc\n","      e_valid_f1 = valid_f1\n","      e_valid_auc = valid_auc\n","      # save best model\n","      torch.save({'epoch': epoch,\n","                  'model_state_dict': model.state_dict(),\n","                  'optimizer_state_dict': optimizer.state_dict(),\n","                  'loss': e_valid_loss,\n","                  }, 'gridsearch-model.pt')\n","  \n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n","  # save metrics for averaging\n","  train_loss_avg.append(e_valid_loss)\n","  train_acc_avg.append(e_valid_acc)\n","  \n","  train_auc_avg.append(e_valid_f1)\n","  train_f1_avg.append(e_valid_auc)\n","\n","  # load best model back\n","  model = LR(EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT).to(device)\n","\n","  best_checkpoint = torch.load('gridsearch-model.pt')\n","  model.load_state_dict(best_checkpoint['model_state_dict'])\n","  optimizer.load_state_dict(best_checkpoint['optimizer_state_dict'])\n","  epoch = best_checkpoint['epoch']\n","  loss = best_checkpoint['loss']\n","\n","  print(model, optimizer)\n","  print(\"Best Epoch {}, Best Validation Loss {}\".format(epoch, loss))\n","  # evaluate on the test set\n","  print(\"Evaluating...\")\n","  test_loss, test_acc, test_f1, test_auc, preds  = evaluate(model, test_loader, criterion)\n","  print(f'\\tTest Loss: {test_loss:.3f} | Train Acc: {test_acc*100:.2f}%')\n","  print(\"Evaluation is Complete!\")\n","  # save test metrics for averaging\n","  test_loss_avg.append(test_loss)\n","  test_acc_avg.append(test_acc)\n","  \n","  test_auc_avg.append(test_f1)\n","  test_f1_avg.append(test_auc)\n","\n","  eval_confusion_matrix = confusion_matrix(test_loader.dataset.tensors[1], preds)\n","  conf_matrices.append(eval_confusion_matrix)\n","# display averaged train and test metrics\n","print('.....')\n","print(f'\\tMean Train Loss: {np.array(train_loss_avg).mean():.3f} |Std Train Loss: {np.array(train_loss_avg).std():.3f}')\n","print(f'\\tMean Train Acc: {np.array(train_acc_avg).mean()*100:.2f}% | Std Train Acc: {np.array(train_acc_avg).std():.3f}')\n","print(f'\\tMean Train F1: {np.array(train_f1_avg).mean():.3f} | Std Train F1: {np.array(train_f1_avg).std():.3f}')\n","print(f'\\tMean Train AUC: {np.array(train_auc_avg).mean():.3f} | Std Train AUC: {np.array(train_auc_avg).std():.3f}')\n","print('.....')\n","print(f'\\tMean Test Loss: {np.array(test_loss_avg).mean():.3f} |Std Test Loss: {np.array(test_loss_avg).std():.3f}')\n","print(f'\\tMean Test Acc: {np.array(test_acc_avg).mean()*100:.2f}% | Std Train Acc: {np.array(test_acc_avg).std():.3f}')\n","print(f'\\tMean Test F1: {np.array(test_f1_avg).mean():.3f} | Std Test F1: {np.array(test_f1_avg).std():.3f}')\n","print(f'\\tMean Test AUC: {np.array(test_auc_avg).mean():.3f} | Std Test AUC: {np.array(test_auc_avg).std():.3f}')\n","print(\"Evaluation is Complete!\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch: 01 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.686 | Train Acc: 55.32%\n","\t Val. Loss: 0.658 |  Val. Acc: 65.15%\n","Epoch: 02 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.655 | Train Acc: 64.09%\n","\t Val. Loss: 0.639 |  Val. Acc: 65.57%\n","Epoch: 03 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.635 | Train Acc: 65.61%\n","\t Val. Loss: 0.625 |  Val. Acc: 66.35%\n","Epoch: 04 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.621 | Train Acc: 66.93%\n","\t Val. Loss: 0.616 |  Val. Acc: 66.72%\n","Epoch: 05 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.611 | Train Acc: 67.60%\n","\t Val. Loss: 0.610 |  Val. Acc: 66.96%\n","Epoch: 06 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.603 | Train Acc: 67.85%\n","\t Val. Loss: 0.605 |  Val. Acc: 67.12%\n","Epoch: 07 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.597 | Train Acc: 68.07%\n","\t Val. Loss: 0.602 |  Val. Acc: 67.21%\n","Epoch: 08 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.592 | Train Acc: 68.33%\n","\t Val. Loss: 0.600 |  Val. Acc: 67.12%\n","Epoch: 09 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.588 | Train Acc: 68.81%\n","\t Val. Loss: 0.599 |  Val. Acc: 67.33%\n","Epoch: 10 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.584 | Train Acc: 69.05%\n","\t Val. Loss: 0.598 |  Val. Acc: 67.50%\n","Epoch: 11 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.581 | Train Acc: 68.97%\n","\t Val. Loss: 0.597 |  Val. Acc: 67.49%\n","Epoch: 12 | Epoch Time: 0m 0s\n","\tTrain Loss: 0.579 | Train Acc: 68.72%\n","\t Val. Loss: 0.598 |  Val. Acc: 67.52%\n","LR(\n","  (fc): Linear(in_features=2004, out_features=1, bias=True)\n","  (dropout): Dropout(p=0.5, inplace=False)\n",") Adam (\n","Parameter Group 0\n","    amsgrad: False\n","    betas: (0.9, 0.999)\n","    eps: 1e-08\n","    lr: 0.01\n","    weight_decay: 0\n",")\n","Best Epoch 10, Best Validation Loss 0.5972378003810134\n","Evaluating...\n","\tTest Loss: 0.725 | Train Acc: 56.59%\n","Evaluation is Complete!\n",".....\n","\tMean Train Loss: 0.618 |Std Train Loss: 0.014\n","\tMean Train Acc: 65.37% | Std Train Acc: 0.016\n","\tMean Train F1: 0.716 | Std Train F1: 0.020\n","\tMean Train AUC: 0.649 | Std Train AUC: 0.008\n",".....\n","\tMean Test Loss: 0.630 |Std Test Loss: 0.064\n","\tMean Test Acc: 65.24% | Std Train Acc: 0.060\n","\tMean Test F1: 0.710 | Std Test F1: 0.080\n","\tMean Test AUC: 0.647 | Std Test AUC: 0.064\n","Evaluation is Complete!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IUMyjqXjgkqq","colab_type":"code","outputId":"f0892172-849d-43b4-ea39-bb08c0597dc6","colab":{"base_uri":"https://localhost:8080/","height":528}},"source":["mean_confusions = np.mean(np.array(conf_matrices), axis = 0)\n","std_confusions = np.std(np.array(conf_matrices), axis = 0)\n","\n","label_names = [\"Laughter\", \"Neutral\"]\n","\n","fig, ax = plt.subplots(figsize = (8, 8))\n","im = ax.imshow(mean_confusions, cmap = \"Blues\")\n","\n","# We want to show all ticks...\n","ax.set_xticks(np.arange(len(label_names)))\n","ax.set_yticks(np.arange(len(label_names)))\n","# ... and label them with the respective list entries\n","ax.set_xticklabels(label_names)\n","ax.set_yticklabels(label_names)\n","\n","# Rotate the tick labels and set their alignment.\n","plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n","         rotation_mode=\"anchor\")\n","plt.xlabel(\"Predicted Labels\", fontsize = 'large')\n","plt.ylabel(\"True Labels\", fontsize = 'large')\n","# Loop over data dimensions and create text annotations.\n","for i in range(len(label_names)):\n","    for j in range(len(label_names)):\n","        text = ax.text(j, i, f'{mean_confusions[i, j]:.3f} ± {std_confusions[i, j]:.3f}',\n","                       ha=\"center\", va=\"center\", color=\"black\")\n","\n","ax.set_title(\"Confusion Matrix\")\n","fig.tight_layout()\n","plt.colorbar(im)\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAiwAAAH/CAYAAAB0NYb1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRETbS8ASERERbe9f2lX2cOhTi3UAAAAASUVORK5CYII=\n","text/plain":["<Figure size 576x576 with 2 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"tyy-Aox3Rnz6","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}